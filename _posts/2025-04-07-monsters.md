---
layout: post
title: "ðŸ²Of Monsters and Metrics:"
subtitle: "Why Measurement isn't Meaning"
---

I originally intended for this to be one single comprehensive discussion addressing a problem I ran into while going about my usual work. That problem has to do with measurement, scale, and taking things at face value.
It has also, as most things I work on, quickly succumbed to scope creep. In my defense, these problems are complex, and the process of solving them requires tacking on layers of abstraction whose output balloons exponentially. "" This specific problem I am going to talk about - we'll dub it the _milk & honey problem_ - is likely similar to issues you have dealt with before, but I'm going to approach it from a slightly different angle. In order to do that , I have divvied this up into 3 separate articles:

1. The setup, background, and conceptual approach that led me to discover the problem.
2. The limitations and pitfalls that exist with the current "standard" tool set.
3. The solution I came up with using frameworks, math, and structural thinking.

- As the well behaved "technical" writer that I am, I will tell you up front that the core problem this series will dig into is this: certain business metrics who share a name and identity (i.e. they consist of the same measurement and unit of observation), may actually be different in practice. In order to relate these measures, gain insight, build models, construct a narrative - all the usual data science stuff  - we need to be able to extract accurate and meaningful information. This isn't an overly deep or complicated problem, at least from a computational perspective. I'm not applying gradient descent to optimize constrained parameters, we're not doing feature engineering, or symbolic regression. At the heart of it, this is really just some multiplication. But it's also about something a lot harder to condense into a neat little package, hence the 3-part blog series. This something is sort of a perspective shift, on the edge of the work that when properly socialized, can have a big impact on the status quo in a data-driven environment. It is also something which I hope will provide value for anyone that comes across it vis-a-vis the framing and method. Finally, this problem relates most fundamentally to statistical inference - regressions, PCA, random forest - conducted on a data point, and how the results of those inferences are affected by the data they ingest.

---
  
## To kick this off, I thought it might help to set the stage with a scenario:

- Imagine you're a wizard tasked by the King with a rather unusual problem: a pack of dragons has taken an inconvenient interest in the local livestock, and he wants you to find a humane way to manage the situation.

- After much alchemical research, you've uncovered a few key facts:

	1. These dragons are conflict-averse. A stern lecture, rooted in logic, should be enough to stop the cow-snacking.
    
	2. But to deliver that message, you'll first need to suppress their flamey glands for at least 30 minutes.
    
	3. You discover two liquids, _milk_ and _honey_, can suppress dragonfire. Some combination of the two will be most effective, but you donâ€™t yet know the ideal ratio.

- Your goal is to find a mix that keeps the dragon from barbecuing cattle long enough for your lecture to work. So, you create a test dragon, submerge it in various milk-honey combos, and track how long the fire stays out.


![Studying Milk and Honey](/assets/images/wizard1.jpg)

- After many days of testing, you make a **startling discovery**:
	- Pure *milk* stops dragonfire for 5 minutes.
	- Pure *honey* stops dragonfire for 15 minutes.
	- **BUT** different ratios of the two stop dragonfire for much longer, anywhere from 15 minutes to an hour!

- But, thereâ€™s a catch: for every gallon of _milk_, the King can only offer a teaspoon of _honey_. The local bees recently unionized and now work 4-hour days.

**The key question is this:**
###  **How do you measure the effect of a mix when its ingredients behave differently ?**

---
## Comparing Apples to Slightly Different Apples

While Iâ€™m going to drop the dragon analogy from here on out (_mostly_), there are a few key ideas this 3-part series is going to explore:

1. **A new way of thinking about business metrics** â€“ Specifically, I want to push you, as a mathy person to think beyond labels and units. What are we _actually_ measuring? And what hidden structural information get neglected when we don't question the assumptions behind a variable?
    
2. **The value of formal frameworks** â€“ Analysis & research in business often skips the step of defining a conceptual foundation. Math is cumbersome, and often not well understood by key leaders in a business environment. But building even a lightweight structural framework, something that clarifies relationships, constraints, and identities, can create a huge amount of value through understanding. Being able to see the language of the problem we are solving changes how we interpret models and improves the kinds of questions we even think to ask.
    
3. **A solution to the _milk & honey_ problem** â€“ Iâ€™ll introduce an approach to combining and analyzing outcomes that seem unrelated and differently scaled, by focusing on their shared structure rather than their surface form. In order to do this, we'll look at one or two of the most common tools for handling scale imbalances to see why this particular problem is better suited for a more subtle approach.

---

## A Brief Tangent on 3 and 4 Letters

I have been working for a while at the intersection of marketing, corporate finance, and data science, and for anyone whose ever spent time working with marketers, you know that they tend to suffer from a bad case of *acronym fever*.

- Unfortunately, the condition is terminal, and in order to treat it we have to address:
	- ROMI
	- ROAS
	- CPGA
	- CPC
	- CPM
	- CPVV
	- CLV
	- etc.

 Not to mention all sorts of short hand terminology we use in the same sentences as all these acronyms. Words like conversion, traffic, door swing, uplift, incremental, impression, indirect, p-max, quality visit and **so** **so** many others.
  
Keeping the definition of all of these straight is hard enough, but actually understanding what they mean is a whole different ball game, or in our case, dragon fight. And as analysts and scientists trying to open the black box of business behavior, we have to take it a step further and apply these metrics in a way that requires deep and fundamental understanding.

---

 While marketing has a particularly persistent case of *acronym fever*, this disease is by no means isolated to the acquisition-oriented. Most professional fields use technical language and abbreviations. (In many ways, marketing KPI's are very similar to financial ratios.)

- In her book *Cultish*, Amanda Montell discusses the use and application of language at length. While I'm not trying to turn this into a linguistics blog (or a dragon fighting one), it's worth quoting her r.e. ðŸª„**jargon**.
	- She breaks jargon into 2 high-level categories:

		1. **In-group signaling** is language we use to fit in. To sound cool, smart, and to signal to others that we are in the proverbial club.
		2. **Expert shorthand** are acronyms and terminology used to simplify and condense complex ideas and concepts to be able to communicate and work at a high level.

	- Generally, marketing acronyms fall into the second category. They serve a purpose by allowing professionals to understand the role and impact that their work is going to have on complex and varied business outcomes.
	- Unfortunately, the irresponsible use of acronyms is contagious and can quickly turn an efficient case *2* to case *1*.

While I'm not here to point any fingers - we all just want to fit in and do well at work - the use of professional jargon can push employees to speak in the same language, even when they might not understand the meaning behind the language they use, in order to fit in and appear skilled. The problems this can cause are worth a blog post in and of themselves[^3]. For now the important thing is understanding how *acronym fever* relates to analytical work. In these contexts, when we measure business performance through KPI's that we source, it is all too easy to overlook the building blocks of these metrics and to take the information they offer at face value.

> If I've learned one thing in trying to improve measurement and business outcomes, it's that **as data experts, we need to be crystal clear in our communication.** 

[^3]: I smell a statistical analysis about the relationship of output quality and acronym prevalence... Hypothesis: $$ \frac{1}{Jargon} âˆ Efficiency $$


---

## Measuring Unalike Metrics

*I can see you, dear reader, on the edge of your seat asking what dragons, milk, and honey have to do with measuring business outcomes, so allow me to indulge you.*

In this example, the problem the data scientist (*wizard*) is trying to solve is measuring the effect of revenue driving activity (*milk and honey*) on business outcomes (*dragon fire & cattle rearing*).

Most product-based businesses have points of sale. These might be physical stores that they own, retail partners (e.g. Walmart), online stores, online merchant platforms (e.g. Amazon), or one of many other forms.

- These businesses primary goal (*normally*) is to sell products.
- In order to sell products, they need to get people into their stores.
	- Where I come from, customers coming to your store is called **traffic**, so that's what I'll use for the rest of this post. While we can measure **traffic** in different ways, the easiest and most clear is just to count *the number of people that come to your store.*
		- For a physical store this is just the number of people that come in the door (and maybe look around a bit).
		- For a digital store this is also just the number of people that come in the *digital* door (go to your website, and stay for some amount of time).

In going about business activity, trying to grow, gain customers, and increase efficiency, measuring traffic is hugely important. **You can't make sales without customers coming in.**


---

## Same-Same but Different

![Lecturing a little dragon](/assets/images/wizard2.jpg)

Maybe you can already where this is going...

- In order to get sales, you need customers to come shop.
- So investing some money in things that will bring potential customers into your stores is probably smart.
- To measure the efficiency of these *traffic* driving investments, you might count the number of sales per customer or customers per sale:

$$
\frac{Sales}{Customer} \leftrightarrow  \frac{Customer}{Sales}
$$

While these ratios are simple, the insight they offer is mighty. The left side tells us:
- What percent of customers coming in make a purchase.
- 
And the right side:
- How many customers need to come through the door to make a sale.

And these are **powerful** metrics. They allow us to build a bridge between the desired *traffic* (people coming to shop) and *sales* (people buying stuff i.e. what you really want). With these beautifully simple ratios you can:

- Figure out how much money you will make for every person that comes in.
- How much you need to invest in things that bring people in to make a lot of money.
- Where and when to invest in these things.
- How to set up your store.
- And much more.

**But there's a catch...**

We don't just sell our products at the corner store. We sell them in a ton of different places. And of those places, we might think to create two categories *web* (digital, online, cyberspace, the world-wide-web...) *store* (store...)

And we care about the people coming to **all** of those places, because **all** of those places is where we sell things. And that's great. The more places you can sell something, the more you'll sell. But this is also the heart of our problem:

*Web traffic is (almost) always significantly higher than in-store traffic*. It's easy to go to a website, you can do it in a few seconds. But comparatively, it's a lot harder to go to a store. While this is fine for treating and measuring effects separately say:

![Distribution of Web and Store performace](/assets/images/rnorm_sw.jpg)

$$
\frac{Sales_W}{Customer_W} \leftrightarrow  \frac{Sales_S}{Customer_S}
$$

### **What do you do when you need to look at things as a whole?**


> This is where I'll leave you. While you wait with bated breath for pt. II of this series, turn the question over in your mind and consider some of the implications this subtle differentiation. What are the properties of these simple components? What problems might arise? What solutions can you think of?

---

If you find yourself too filled with anticipation to wait, or you want to lodge a complaint, reach out over on my [Contact Page](https://benjaminpharris.github.io/contact/). You can also jump ahead and see [Example Code](https://github.com/benjaminpharris/benjaminpharris.github.io/tree/main/Example%20Code/MonstersAndMetrics) on my [Github](https://github.com/benjaminpharris/benjaminpharris.github.io)
