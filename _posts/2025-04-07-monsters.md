---
layout: post
title: "Of Monsters and Metrics: Wrangling Chaos into Coherent KPIs"
---


# **ðŸ² Of Monsters and Metrics**
## Wrangling Chaos into Coherent KPIs

**This blog post covers a novel solution for comprehensive analysis against outcomes that vary across orders of magnitude. I walk through the process of developing a theoretical framework - a system for thinking about data, and then apply it to simulated business data in Python.

---
  
In thinking about how to best kickoff this blog post, I thought it might be good to set the stage with a scenario. I apologize in advance for the convoluted analogy, but I think it will easily get the point across, and set us up to understand the problem.

![[Pasted image 20250407160236.png]]

- **To start,** let's imagine you are a wizard who has been tasked by the King with finding a solution to a local dragon which has recently taken interest in the local livestock.

- After much alchemical research, you have discovered that two particular liquids are effective at suppressing dragon fire - *milk and honey* - but you don't yet know the most effective ratio to combine the two. 

- You want to find the quantity that is going to prevent the dragon from cooking anymore cows long enough to give him a stern talking to and explain the error of his ways. So in order to do this, you have created a test dragon that you submerge in various combinations of milk and honey and track the amount time the dragonfire stops.

- After many days of testing, you make a **startling discovery**:
	- Pure *milk* stops dragonfire for 5 minutes
	- Pure *honey* stops dragonfire for 10 minutes
	- Different ratios of the two stop dragonfire for much longer, anywhere from 15 minutes to an hour!

- To make matters even worse, for every gallon of *milk* the king can provide, he can only give you a teaspoon of *honey*. 
	- This means that the mix is always much more milk than honey even though honey is much more effective.

#### The question is this:

#### How do you measure the effect of a mix, when the parts of that mix **behave differently** and are **different in size**?

#### *Stay tuned to find out...*

---

## A Brief Inquiry Into 3 and 4 Letters

I have been working for a while in marketing measurement and data science, and for anyone whose ever spent time in the space, you know that marketing as an industry tends to suffer from a bad case of *acronym fever*.

- Unfortunately, the condition is terminal, and in order to treat it we have to address:
	- ROMI
	- ROAS
	- CPGA
	- CPC
	- CPM
	- CPVV
 - ... Just to name a few - not to mention all sorts of short hand terminology we use for business metrics. Words like conversion, traffic, door swing, uplift, incremental, impression, indirect, p-max and **so** **so** many others.

  
Keeping the definition of all of these straight can be hard enough, but actually understanding what they mean is a whole different ball game, or in our case, dragon fight.  

---

While marketing has a particularly bad case of *acronym fever*, this disease is by no means isolated to the acquisition-oriented. Most professional fields use technical language and abbreviations. (In many ways, marketing KPI's are very similar to financial ratios.)[^3]

- In her book *Cultish*, Amanda Montell discusses the use and application of language at length. While I'm not trying to turn this into a linguistics blog (or a dragon fighting one), it's worth quoting her r.e. ðŸª„**jargon**.
	- She breaks jargon into 2 high-level categories
		1. **In-group signaling** is  language we use to fit in, to sound cool, smart, and to signal to others that we are in the proverbial club.
		2. **Expert shorthand** is acronyms used to simplify and condense complex ideas and concepts to be able to communicate and work at a high level.

	- Generally, marketing acronyms fall into the second category. They serve a purpose by allowing professionals to understand the role and impact that their work is going to have on complex and varied business outcomes.
	- Unfortunately, the use of acronyms can also cause a shift from case *2* to case *1

While I'm not here to point any fingers - we all just want to fit in and do well at work - the use of professional jargon can push employees to speak in the same language, even when they might not understand the meaning behind the language they use.

##### I apologize for the preamble and extensive background, but if I've learned one thing in trying to improve measurement and business outcomes, it's that **as data experts, we need to be crystal clear in our communication.** And this background is super important for digging into the meat of the issue. So let's get into it...

[^3]: I smell a statistical analysis in there about the relationship of output quality and acronym prevalence...

---

### Measuring Unalike Metrics

I can see you dear reader leaning on the edge of your seat waiting to hear more about what dragons and milk and honey have to do with measuring business outcomes, so allow me to indulge you.

- Most product-based businesses have points of sales. These might be physical stores that they own, retail partners (like Target or Walmart), online stores, online retail partners (like Amazon), or they might take one of many other forms.

- These businesses primary goal (*normally*) is to sell products. In order to do that, they need to get people into their stores

	- Potential customers coming to your store is called **traffic**.  While we can measure **traffic** in different ways, the easiest and most clear is just measuring *the number of people that come to your store.*
		- For a physical store this is just the number of people that come in the door (and maybe look around a bit).
		- For a digital store this is also just the number of people that come in the *digital* door (go to your website).

In going about business activity, trying to grow, gain customers, and increase efficiency, measuring traffic is hugely important. **You can't make sales without customers coming in.**

---

## Same-Same but Different

Maybe you can already where this is going-

- In order to get sales, you need customers to come shop
- To measure the efficiency of your business, you might count the number of sales per customer or customers per sale

$$
\frac{Sales}{Customer} \leftrightarrow  \frac{Customer}{Sales}
$$
**But...**

Web traffic will almost always be significantly higher than in-store traffic. And while this is fine for treating and measuring effects separately say:

$$
\frac{Sales_D}{Customer_D} \ne  \frac{Sales_S}{Customer_s}
$$

**What do you do when you want to look at things as a whole?**

> From here on out, we're going to get into the meat of things, but don't be too afraid, you can always reach out over on my [Contact Page](https://benjaminpharris.github.io/contact/).

---

### 1. Outcomes at Scale -

- For the majority of this analysis, we're going to be talking in the context of absolute measurements. While simple scaling / centering is always an option, business analysis often need concrete outputs that are interpretable in the context of tangible values like *sales, traffic, and dollars*.
	- Measurements of standard deviation can have challenging interpretations, especially across disparate variable scales.
	- This is made even worse when you look at transformed inputs and outputs in estimated models where marginal effects are non-linear and dependent on multivariate inputs
- Whenever possible, clear interpretation that doesn't require reversing complex transformations are generally preferred.[^1] While this isn't a universal axiom (it would be a pretty bad one), interpretable metrics are definitely a plus.[^2]

[^1]: It's worth mentioning that the context of this is in that of another project soon to debut comparing model fit and backtesting efficiency across 6 estimation techniques.

[^2]: I hear your internal screaming, but don't worry, we'll talk all about the problems with scaling here in just a minute.
- So that being said, let's get some setup for our two key variables: *traffic* and *sales*

>To start we create 1,000 observations of sales and marketing data using Numpy's random.normal() function. These are distributed at the scales we might see in a business.

```
np.random.seed(42)

dates = pd.date_range(start="2022-01-01", periods=1000, freq="D")

# Sales Data
store_sales = np.random.normal(loc=4500, scale=500, size=1000).clip(3000, 6000)
web_sales = np.random.normal(loc=3500, scale=600, size=1000).clip(2000, 5000)

# Traffic Data
store_traffic = np.random.normal(loc=5000, scale=1000, size=1000).clip(2000, 8000)

web_traffic = np.random.normal(loc=2_000_000, scale=1_000_000, size=1000).clip(500_000, 5_000_000)

# Create a dataframe
df = pd.DataFrame({
Â  Â  'date': dates,
Â  Â  'store_sales': store_sales.astype(int),
Â  Â  'web_sales': web_sales.astype(int),
Â  Â  'store_traffic': store_traffic.astype(int),
Â  Â  'web_traffic': web_traffic.astype(int)
})
```

- The basic summary stats and distribution from `df.describe()` looks like:

![[Pasted image 20250407153030.png]]

- And a quick plot in `seaborn` using `sns.histplot()`

![[Pasted image 20250407153320.png]]

- Notice the `scale` argument to `np.random.normal()` creates delicious noisy variation.
	- In a more rigorous context we might want our data to trail off more smoothly, for our example context this is perfectly fine.


### 2. Truth is subjective - 

Just as a quick refresher we're trying to understand the correct mix of *milk & honey* to tame the dragon aka the effect of our marketing (or other business activity) on ***digital and store traffic*** as whole. We need the full mix because:
1. We don't know which of our activities target digital and which target our stores and
2. Even if we did, business activity happens holistically, affecting both digital and store activity
3. We want a big picture perspective of how our business is doing

While the statisticians among you might immediately jump to centering and scaling as a cure all here, we're trying to retain interpretability across estimation strategies, and hack together some clean output we can put on an executives desk to say:

*"This is what happens to traffic when we change these variables"*

The more stubborn among the statisticians might still be annoyed. After all, it's our job to interpret results, and translating a measure of standard deviation into actual outcome values is a pretty trivial matter. But that only solves the problem on paper!

- When our outcomes differ in orders of magnitude, we can easily scale them. For example, using `sklearn.preprocessing`  `StandardScaler()`.
- And beautifully, calling the scaler function on our dataframe preserves all that beautiful variation, but now gives a constant scale across our different outcomes.

![[Pasted image 20250407154840.png]]

- Now we can do all that fun analytical work. Our variables are the same scale and we can combine them into one holistic *traffic* measure...

### 3. Linear Transformations are ... Linear - 

Let's look at the output of some simple regressions on transformed and non-transformed data and see what happens

- For the sake of the other things I need to get done this week, I'm going to skip over the code and output of that work but here's a quick brief:
	1. Create 8 predictor variables to represent the spending on different parts of our business activity
	2. Combine our web and store traffic into a single *traffic* variable
	3. Combine scaled traffic into a single *scaled_traffic* variable
	4. Regress our combined traffic variables on our predictors
