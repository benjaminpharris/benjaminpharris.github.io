---
layout: post
title: "The Trouble with Traffic"
subtitle: "Why Measurement isn't Meaning"
---
![The Black Box](/assets/images/wizmuseum.jpg)

---
> (The views expressed here are my own, written outside work hours, and not affiliated with any organization. All data simulated.)

---


> *(The views expressed here are my own, written outside work hours, and not affiliated with any organization. All data simulated.)*

It's been a while since my last blog post... The river of data never stops flowing.ðŸšªNew doors to open, ðŸ—ï¸ new keys to try. And while I'd love to believe that all of you have been deeply considering the problem I presented last time, for all of our sakes, I'll open with a refresher.

### **On last month's episode of the data blog....**

We met the wizard, the king, and his dragons. The wizard had been assigned the job of figuring out how to stop a flock (pack?) of dragons from devouring the kingdom's livestock. After a lot of research, the wizard realized that the best solution was to bathe the dragons in a mixture of milk and honey[^6]. The problem was that he wasn't sure how to figure out the optimal ratio of both because the *net treatment effect* of the mixture was different from the *linear combination* of the individual effects of milk and honey.

In the real-world context of business metrics, we said we were trying to measure the effect of business driving activity on the number of people that come check out our stores. But we have *digital* and *physical* stores, and the magnitude of traffic between the two is wildly different.

![Some Distributions](/assets/images/rnorm_sw.jpg)

- Note the little *1e6* on the web traffic $x$ axis $\rightarrow$ millions per day to our thousands of in-store visitors.

---

## **A Relaxing Stroll Through the Solution Space**

There are probably dozens of potential ways to handle these types of **scale imbalances**, each with benefits and drawbacks. Before we discuss the solution I'm offering to this problem, we will talk about a few of those. I am by no means claiming all-encompassing knowledge of the out-of-the-box methods that exist for *scaling* data, or the ability to give you a solution to all your problems[^7]. But what I can do is take us on a walk through the museum of data transformations, show some of the exhibits I like, and point out a thing or two you might not have noticed before. 

Data oriented persons in the audience may have various levels of familiarity with these approaches. I've certainly added a few new ones to my toolkit in the last couple of weeks. But my goal here, more than pointing to new tools you can add to your toolkit, is to advocate for my own personal philosophy on the *use* of these tools, which itself is sort of an input to the **approach** I'm introducing in this series.

### ðŸ“Š Donâ€™t Judge a Distribution by Its Mean 

So far weâ€™ve talked about the problem in a colloquial way. If Iâ€™ve done my job, the inherent issue should be reasonably clear: **scale imbalances**. But Iâ€™d wager that at least some of you are still fuzzy on whatâ€™s _actually_ going on and why it matters.

_If only we had a tool that could clarify complex dynamics in a precise, unambiguous wayâ€¦_

**âœ¨MATHâœ¨**

Cue the trauma. For many, math evokes visceral discomfort. For others, itâ€™s not math itself, but the way we first encountered it that leads us to label ourselves **ðŸ·ï¸ bad at math**.

First impressions matter. And the first impression many of us get is gray worksheets, lame cartoons, and problems like how many apples Lisa needs to give Bob for an eraser. Itâ€™s math as dull ritual, taught by someone we didnâ€™t choose, in a room we didnâ€™t want to be in.

Worse, the â€œrelatableâ€ framing: apples, trains, erasers, is so transparently unrelatable that even as kids we sensed the desperation. It wasn't just boring,  it was insultingly boring.

But thatâ€™s not the real problem. We grow out of broccoli-as-airplane. We donâ€™t grow into math. Only _**9% of U.S. adults are proficient in numeracy**_.[^8] So what gives?

The deeper issue is **how we frame math**. Not just to children, but to everyone. When we say â€œmath,â€ most people think: algebra, equations, Pythagorean theorem, tests, memorization. We associate it with school, with failure, with a set of arbitrary procedures to solve meaningless problems.

And thatâ€™s the disconnect.

Because math isnâ€™t a set of procedures.

**Math is a language.**  
A language we can use to describe systems, relationships, and behaviors, from the atomic to the astronomical, clearly and consistently. And when you begin to see math this way, not as something to memorize but something to _say something with_, it stops being about erasers and starts being about insight.

### ðŸ–¼ï¸ Consider the Frame

My first instinct whenever I am confronted with a new analytical problem is to understand the dynamics of the system. This is a reframe on  a perspective that is common in the world of predictive modeling. The question data scientists often ask is *"What factors will give my model the best fit?"*

I can't blame them. Model fit - $ r^2 $, MAPE, SHAP, %MSE - represent a quantifiable way to assess the performance of a model. But there's an assumption here, an implicit belief. 

> **If I can predict the outcome with 100% accuracy, then I have accurately modeled the system.**

This goes back to the data science question - *what are the important factors*? You consider  the system you're interested in, the process your modeling. Looking at some stock prices? Include inflation, interest rates, earnings, maybe GDP. You end up trying to figure out what factors need to be included to get the result. Because the objective is to predict the future.

That's a powerful thing in business, if you have a good idea about what will happen, you gain first mover advantage. And if you choose a set of inputs, mash them through a model, and end up with a line that does a great job following the outcome, you have done just that.

But the problem with this approach is that it neglects some very important details about model creation. 

$$
Model~Fit \ne Causal~Impact \leftrightarrow Correlation \ne Causation
$$

High fit doesn't necessarily mean **good** fit. It means that you have successfully created a recipe that gives the desired result.You stuck some things together in a pot that came out tasting like soup. But if you donâ€™t know what made it taste good, how will you scale the recipe? Or fix it when something spoils?

---

To make this a little more concrete, let's look at three areas where model fit fails to accurately communicate the true quality of a model:

#### **1. Overfitting** 

Of the issues in predictive modeling, this one is probably the most talked about, and the most easily addressed. Here are three different endogenous variables we are interested in including in a model:

![Scatterplot](/assets/images/gpr.jpg)

We can see that each exhibits nice variation across it's range of values. We would expect that each of these features is contributing to our outcome in some way - that's why we chose them. 

![[Pasted image 20250519132629.png]]
But when we go to put this data into a model (GPR in this case), we can see that the first feature ends up with a huge amount of the variation, while the other two end up with  very little of the net effect in spite of the fact that visually, they all exhibit about the same amount of dispersion.

In open parametric models (like OLS) where we have estimation error and marginal effects, it's a little easier to identify when this is going on. But in the case of GPR and other black box / non-parametric models,  especially ones that use piecewise estimation, overfitting can be harder to identify if you only look at standard diagnostics like loss curves and model fit.

Even worse, the overfitting may happen more subtly across features. A quote from one of my graduate professors perfectly captures this:

>*"People tend to think that low $r^2$ like 10% means that the model is bad. But maybe that predictor only contributes to 10% of the outcome."*

Model fit doesn't tell us anything about how true our model is. It just tells us how well we are able to fit a line. If you know anything about Fourier analysis, you probably have a good intuition about how this choosing features approach can lead to overfitting. 

If not, that's okay. The important thing to understand is that the more variables we put into a model, the more flexible that model becomes. More variables means a more squiggly line. And because the way these models usually work is by minimizing some distance, we end up "bending" the line more and more as we throw more ingredients to our soup.
#### **2. Out of sample prediction**

The next problem is also one that a lot of black box models attempt to handle through a process called cross validation. I have another quote from the same professor that fits in well here (even though it was in the context of imputation):

> *"We can try to guess what happened at the point we don't observe based on the context around it. But the problem is that it's a guess. And we have no way of knowing if the pattern around it holds up or if something entirely different happened."*

This is at the heart of out of sample prediction. Maybe the best way to illustrate this is using different observed samples of the logistic curve. Using the buttons in the figure below, you can move through different samples taken from the logistic curve. (I spent way too much time getting Plotly to work here so please ooh and ahh at my clickable buttons.)

<iframe 
  src="{{ '/assets/plots/out_of_sample_viz.html' | relative_url }}" 
  width="100%" 
  height="600" 
  frameborder="0"
></iframe>

- Using the different buttons at the top of the figure, you can see the way that a simple linear trend fits to a subsample of the data. 
- Examining the fit of any of the lines here, they seem like quality estimates, but when  combined it becomes clear that these lines fail to capture anything meaningful  about the underlying relationships. 

![Out of sample oops](/assets/images/oos.jpg)

For each of these subsamples, their local trends -*in-sample predictions*- are reasonable. But  when we extend those lines -*out-of-sample*- as we might do when running scenario analysis for a business, we can see how that the predictions move wildly off scale.

In our context, we might think of the overall logistic curve as **milk & honey**, or more precisely **the dragon's response to milk & honey** and the individual subsamples as the observations we make when testing different combinations individually.

We can see that we need more than just the right tool to be able to understand the underlying system and act based on it's dynamics. What we're trying to do is not just measure and predict effects, but create a representation of the system at large.

#### **3. Interpretability**

The last problem with relying solely on model fit is one which I have been coming to understand more and more as I interact with data scientists with varied backgrounds. As I have already been hinting at, we can break modeling down into two high level categories:
1. **Predictive Modeling**
2. **Causal Inference**

In the data science world, the former is generally more common. Predictive modeling is really the thing that drives most of the business world. ðŸ”® Telling the future. And when you use Python, the data  science landscape revolves heavily around these models. Things like:

- Spline Regression
- Random Forest
- Gradient Boosting
- Shallow and deep neural networks

Models that rely heavily on machine learning methods  are king in the world of data science because they do a great job at handling fit issues **1** and **2** (when correctly configured). But the problem is that these models aren't built to tell us anything about the underlying relationship.

In Scikit-Learn and PyTorch, the *mlpregressor* (neural network) pipeline gives a model object as an output. And while, with some effort, you can visualize the partial dependence of the different features you  give to those models, there isn't a clean mathematical equation you can write down to describe the relationship between the inputs and the outputs.

The way these models are designed you:

$$
Train \rightarrow Refine \rightarrow Predict
$$

You configure your model and all of the nasty details like cross validation and hyperparameter search, examine the quality of the result and make adjustments to your configuration, and then stick in new data to try and figure out how things will look.

Black-box models are effectively really complex scenario analysis tools. You give it some historical information, feed it a new scenario, and it tells you what it thinks will happen. If that's ringing a bell, that's because it's the same architecture modern AI comes from. They are just really good prediction engines.

And while in our case, you can look at the shape of the in-sample response curves by plotting partial dependence[^9], you can't write down or describe the relationships that these models predict. They are just playing connect the dots in like 12 dimensions.

---

### â˜¯ï¸ The Zen of Modeling

All of these problems arise from models whose goal is to make a prediction without describing a mathematical relationship. 

They trade accuracy in the form of *goodness of fit* for interpretability. They say "I don't care about what's actually happening, I just want to guess at what will happen next." And that can be a really powerful thing sometimes. These models can do a really good job of providing predictions and describing systems, especially when the margin of what you're looking at is pretty close to what's happened historically. They capture non-linearity elegantly, and they *can* (big emphasis there) remove a lot of human fallibility from the modeling and measurement pipeline.

But they also fail to do the thing we are really after which is never really just prediction. We might say we want a forecast, but unless we actually truly understand the dynamics of the system, we can't say with confidence will happen next. The only way we can do that is by accurately and completely capturing the mathematical relationship. Because that relationship is what's actually going on. It's the **truth**. It defines what happens. The mathematical relationship tells us when the logistic curve flattens out, when our returns diminish, and when they're at they're strongest. If we get the causality correct, we will always get the prediction right. But getting the prediction right today, without understanding the causality, means that our prediction will eventually - tomorrow, or a week, or a month from now - be wrong.

And that gets close to the thing I'm trying to impart unto thee. Though it's not this economist looking down there nose at you causal inference is superior attitude. It's the belief that, as data people, our principle job is always to understand the dynamics of the system, and to choose our tools based on our understanding of those dynamics.

What was I talking about? Oh right, data scaling. Well, that will have to be next time given the tangent we had to go on. But before I go, I want to say one more thing r.e. prediction vs. causality.

![Something Zen](/assets/images/koanmodel.jpg)

There's this whole Zen Koan data thing we can talk about when debating whether we should always bring a causal lens to a problem. I'm going to abuse a very overused quote here *"All models are wrong, some are useful"*[^10]. 

While many a statistician has used this to get a laugh out of a crowd before a very dry presentation on multinomial logit or proofs using way too many conditional probabilities, it's not just a cute cliche. It is very much a true statement. No matter how hard we try, the amount of inputs that truly affect the output of a system can never be written down. There will always be chaos in the world. In fact, the nature of causality is still up for debate entirely, but even less existentially, a model is always a simplification. 

Another way of saying all that - ***most relationships aren't purely algebraic.*** A lot of things probably can't be decomposed into a set of equations that completely describes their behavior. And in that sense, whether we choose to put our faith in causality, the non-parametric ambiguity of the black box might actually be the best description of the world. 

---

### ðŸ§  I'll leave you with this:

> _"Instead of attempting such a definition [of 'thinking'], I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words. The new form of the problem can be described in terms of a game... Are there imaginable digital computers which would do well in the imitation game?"_  
> â€” Alan Turing, _Computing Machinery and Intelligence_

This quote is from the seminal paper that introduced the Turing test. Itâ€™s powerful because it highlights how even Turing dodged the hard philosophical question _what is thinking_ by reframing it around observable behavior.

At some point, prediction becomes performance. A model that fits the past isn't the same as a model that understands the system. But the seduction of predictive success is strong, especially when you're paid to be right, not deep. Still, there's a reason we keep pushing the goalposts on AI. The Turing Test used to be our benchmark for intelligence, until we realized that fooling us isnâ€™t the same as thinking. Just like forecasting isn't the same as knowing. So maybe the next test isnâ€™t imitation, but explanation. Can your model not only guess what happens next, but say why?


[^6]: Go read the last blog post if you're confused...

[^7]: Consider therapy for that.

[^8]: From like 3/4 of people I've talked to and a quick google search https://hbr.org/2019/10/americans-need-to-get-over-their-fear-of-math?utm_source=chatgpt.com

[^9]: The non-parametric equivalent of a partial derivative. Basically the line that one input predicts for the output.

[^10]: Attributed to the late George Box.


---

If you find yourself too filled with anticipation to wait, or you want to lodge a complaint, reach out over on my [Contact Page](https://benjaminpharris.github.io/about/). You can also jump ahead and see [Example Code](https://github.com/benjaminpharris/benjaminpharris.github.io/tree/main/Example%20Code/MonstersAndMetrics) on my [Github](https://github.com/benjaminpharris/benjaminpharris.github.io)
